{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final-clean.txt\",\"r+\") as f:\n",
    "    a=f.read()\n",
    "#a.replace(\"sqrt\",\"\")\n",
    "#a.replace(\"phi\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590084\n"
     ]
    }
   ],
   "source": [
    "print(len(a))\n",
    "a=a.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=len(a)/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98343.83333333333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98343\n",
      "590063\n",
      "590063\n",
      "590063\n",
      "590063\n",
      "590063\n",
      "590063\n"
     ]
    }
   ],
   "source": [
    "z=[]\n",
    "t=0\n",
    "print(int(t+b))\n",
    "for _ in range(5):\n",
    "    z.append(a[t:int(t+b)])\n",
    "    t=int(t+b)\n",
    "z.append(a[t:])\n",
    "# print(z)\n",
    "for m in z:\n",
    "    print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/wanderer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'couldn', 'themselves', 'but', \"won't\", 'isn', 'are', \"isn't\", \"should've\", 'what', 'from', 'while', 'more', 'y', 'ma', 'during', 'as', 'your', 'being', 'before', 'itself', 'only', 'once', \"didn't\", 'yourself', \"mustn't\", 'after', 'with', 'haven', 'than', 'have', \"haven't\", \"you'll\", \"needn't\", 'until', 'mustn', \"you'd\", 'am', 'by', 'mightn', \"shouldn't\", \"it's\", 'yourselves', 'below', 'up', 'we', 'any', 'i', 'very', \"doesn't\", 'or', 'a', 'our', 'out', \"weren't\", 'will', 'such', 'why', 'doesn', 't', 'in', 'll', 'who', 'ain', 'these', 'their', \"don't\", \"shan't\", \"that'll\", \"you've\", 'doing', 'so', 'all', 'shouldn', 'she', 'was', 'again', 'few', 'can', 'd', 'they', 'hers', 'an', 'here', 'aren', 'both', 'herself', 'down', \"wasn't\", 'nor', 're', 'her', 'where', 've', 'same', 'were', 'off', 'the', 'too', 'about', 'now', 'weren', 'be', \"she's\", 'had', 'which', 'wasn', 'most', 'o', 'between', 's', 'should', 'yours', 'how', 'against', 'don', 'did', 'hadn', 'me', 'he', 'needn', 'above', 'do', 'just', 'been', 'hasn', 'didn', 'not', 'does', 'no', 'this', 'there', 'under', 'them', \"mightn't\", 'then', 'my', 'his', 'of', \"couldn't\", 'own', 'whom', 'because', \"you're\", 'those', 'on', 'if', \"wouldn't\", 'some', 'shan', 'and', 'won', 'is', 'it', 'to', \"aren't\", 'through', 'theirs', \"hasn't\", 'himself', 'ourselves', 'at', 'having', 'into', 'm', 'has', 'wouldn', 'over', \"hadn't\", 'further', 'you', 'that', 'for', 'when', 'him', 'its', 'each', 'ours', 'myself', 'other'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(stopWords)\n",
    "stemmer=SnowballStemmer(\"english\")\n",
    "# stopwords.push(2)\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def clean(document):\n",
    "    tokens = RegexpTokenizer(r'\\w+').tokenize(document.lower())\n",
    "    tokens_clean = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    tokens_stemmed = [lemmatize_stemming(token) for token in tokens_clean]\n",
    "    return (tokens_stemmed)\n",
    "\n",
    "stack_articles_clean = list(map(clean,z))\n",
    "#print(stack_articles_clean)\n",
    "for _ in stack_articles_clean:\n",
    "    while \"answer\" in _:\n",
    "        _.remove(\"answer\")\n",
    "    while \"test\" in _:\n",
    "        _.remove(\"test\")\n",
    "    while \"sqrt\" in _:\n",
    "        _.remove(\"sqrt\")\n",
    "    while \"ghost\" in _:\n",
    "        _.remove(\"ghost\")\n",
    "    while \"butterfli\" in _:\n",
    "        _.remove(\"butterfli\")\n",
    "    while \"phi\" in _:\n",
    "        _.remove(\"phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "article_contents = [article for article in stack_articles_clean]\n",
    "print(len(article_contents))\n",
    "dictionary = corpora.Dictionary(article_contents)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in article_contents]\n",
    "print(len(bow_corpus))\n",
    "bow_doc_4310 = bow_corpus[0]\n",
    "# Output of words along with their frequency.\n",
    "# for i in range(len(bow_doc_4310)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "#                                                dictionary[bow_doc_4310[i][0]], \n",
    "# bow_doc_4310[i][1]))\n",
    "# for i in range(len(dictionary)):\n",
    "#     print(dictionary[i])\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "# Sample ouptput of doc\n",
    "# for doc in corpus_tfidf:\n",
    "#     pprint(doc)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.000*\"psi\" + 0.000*\"frac\" + 0.000*\"haag\" + 0.000*\"capsul\" + 0.000*\"period\" + 0.000*\"pariti\" + 0.000*\"rangl\" + 0.000*\"valenc\" + 0.000*\"bcs\" + 0.000*\"dagger\"\n",
      "Topic: 1 Word: 0.000*\"finger\" + 0.000*\"monopol\" + 0.000*\"card\" + 0.000*\"strategi\" + 0.000*\"chamber\" + 0.000*\"pretend\" + 0.000*\"winner\" + 0.000*\"sensor\" + 0.000*\"pt\" + 0.000*\"prevent\"\n",
      "Quantum Mechanics\n",
      "[[(0, 0.74631846), (1, 0.25368154)]]\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus_tfidf, num_topics=2, id2word=dictionary, passes=400)\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "# pprint(lda_model.print_topics())\n",
    "# Bus\n",
    "print('Quantum Mechanics')\n",
    "print( list( lda_model[ [dictionary.doc2bow(article_contents[0])] ]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
